{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minsu/.conda/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-24 01:10:47.772956: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-24 01:10:47.822259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-24 01:10:48.527565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from model import BertForQuestionAnswering, RobertaForQuestionAnswering\n",
    "from dataset import QADataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_dir = \"data/aihub_administration/test.json\"\n",
    "context_dir = \"data/aihub_administration/context.json\"\n",
    "\n",
    "pretrained_models_dir = \"result/roberta/aihub_administration_1024/\"\n",
    "\n",
    "batch_size = 64\n",
    "max_length = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_models = sorted([pretrained_model for pretrained_model in os.listdir(pretrained_models_dir) if \"runs\" not in pretrained_model])\n",
    "context = json.load(open(context_dir, 'r'))\n",
    "preds = []\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_models_dir + pretrained_models[0])\n",
    "\n",
    "for pretrained_model in pretrained_models:\n",
    "\n",
    "    pretrained_model = pretrained_models_dir + pretrained_model\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "    test_set = QADataset(data_dir=test_dir, context=context, tokenizer=tokenizer, max_length=max_length)\n",
    "\n",
    "    if config.model_type == \"bert\":\n",
    "        model = BertForQuestionAnswering.from_pretrained(pretrained_model)\n",
    "    elif config.model_type == \"roberta\":\n",
    "        model = RobertaForQuestionAnswering.from_pretrained(pretrained_model)\n",
    "\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir = \"tmp/\",\n",
    "        overwrite_output_dir = True,\n",
    "        do_predict = True,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        logging_steps = 10,\n",
    "        seed = 42,\n",
    "        data_seed = 42,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer,\n",
    "    )\n",
    "\n",
    "    preds.append(trainer.predict(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_(text):\n",
    "        ''' 불필요한 기호 제거 '''\n",
    "        text = re.sub(\"'\", \" \", text)\n",
    "        text = re.sub('\"', \" \", text)\n",
    "        text = re.sub('《', \" \", text)\n",
    "        text = re.sub('》', \" \", text)\n",
    "        text = re.sub('<', \" \", text)\n",
    "        text = re.sub('>', \" \", text)\n",
    "        text = re.sub('〈', \" \", text)\n",
    "        text = re.sub('〉', \" \", text)\n",
    "        text = re.sub(\"\\(\", \" \", text)\n",
    "        text = re.sub(\"\\)\", \" \", text)\n",
    "        text = re.sub(\"‘\", \" \", text)\n",
    "        text = re.sub(\"’\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(remove_(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    #F1 by character\n",
    "    prediction_Char = []\n",
    "    for tok in prediction_tokens:\n",
    "        now = [a for a in tok]\n",
    "        prediction_Char.extend(now)\n",
    "\n",
    "    ground_truth_Char = []\n",
    "    for tok in ground_truth_tokens:\n",
    "        now = [a for a in tok]\n",
    "        ground_truth_Char.extend(now)\n",
    "        \n",
    "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_Char)\n",
    "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint-147611] em score: 72.8826\tf1_score: 90.1524\n",
      "[checkpoint-163149] em score: 72.732\tf1_score: 90.1227\n",
      "[checkpoint-170918] em score: 72.6915\tf1_score: 90.2541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, pred in enumerate(preds):\n",
    "\n",
    "    predictions = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    exact_match = 0\n",
    "    f1 = 0\n",
    "    total = 0\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for sample_idx, sample in enumerate(test_set.samples):\n",
    "\n",
    "        pred_start = predictions[0][sample_idx].argmax(-1)\n",
    "        pred_end = predictions[1][sample_idx].argmax(-1)\n",
    "\n",
    "        pred_text = tokenizer.decode(test_set[sample_idx][\"input_ids\"][pred_start:pred_end], skip_special_tokens=True)\n",
    "\n",
    "        label_text = context[sample[\"context\"]][sample[\"answer_start\"]:sample[\"answer_end\"]]\n",
    "        \n",
    "        exact_match += exact_match_score(pred_text, label_text)\n",
    "        f1 += f1_score(pred_text, label_text)\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    exact_match = round(100.0 * exact_match / total, 4)\n",
    "    f1 = round(100.0 * f1 / total, 4)\n",
    "\n",
    "    print(\"[{}] em score: {}\\tf1_score: {}\".format(pretrained_models[i], exact_match, f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint-147611] > 512 em score: 79.2324\tf1_score: 94.0733\n",
      "n samples: 1459\n",
      "[checkpoint-163149] > 512 em score: 79.5751\tf1_score: 94.6677\n",
      "n samples: 1459\n",
      "[checkpoint-170918] > 512 em score: 80.0556\tf1_score: 95.0895\n",
      "n samples: 1439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, pred in enumerate(preds):\n",
    "\n",
    "    predictions = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    exact_match = 0\n",
    "    f1 = 0\n",
    "    total = 0\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for sample_idx, sample in enumerate(test_set.samples):\n",
    "\n",
    "        pred_start = predictions[0][sample_idx].argmax(-1)\n",
    "        pred_end = predictions[1][sample_idx].argmax(-1)\n",
    "\n",
    "        if pred_start >= 512 and pred_end >= 512:\n",
    "\n",
    "            pred_text = tokenizer.decode(test_set[sample_idx][\"input_ids\"][pred_start:pred_end], skip_special_tokens=True)\n",
    "\n",
    "            label_text = context[sample[\"context\"]][sample[\"answer_start\"]:sample[\"answer_end\"]]\n",
    "            \n",
    "            exact_match += exact_match_score(pred_text, label_text)\n",
    "            f1 += f1_score(pred_text, label_text)\n",
    "\n",
    "            total += 1\n",
    "\n",
    "    exact_match = round(100.0 * exact_match / total, 4) if total != 0 else 0.\n",
    "    f1 = round(100.0 * f1 / total, 4) if total != 0 else 0.\n",
    "\n",
    "    print(\"[{}] > 512 em score: {}\\tf1_score: {}\".format(pretrained_models[i], exact_match, f1))\n",
    "    print(\"n samples: {}\".format(total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
